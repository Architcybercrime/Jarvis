from fastapi import FastAPI
from pydantic import BaseModel
import joblib
import datetime
import traceback

app = FastAPI(title="ü§ñ JARVIS API")

# Global variables for models (lazy load)
model = None
vectorizer = None

def load_models():
    global model, vectorizer
    try:
        model = joblib.load('models/intent/intent_classifier.pkl')
        vectorizer = joblib.load('models/intent/tfidf_vectorizer.pkl')
        print("‚úÖ Models loaded!")
        return True
    except Exception as e:
        print(f"‚ùå Model load error: {e}")
        return False

class Query(BaseModel):
    text: str

def predict_intent(text):
    if model is None or vectorizer is None:
        return "error"
    try:
        return model.predict(vectorizer.transform([text]))[0]
    except:
        return "unknown"

def handle_intent(intent):
    responses = {
        'greeting': '‡§®‡§Æ‡§∏‡•ç‡§§‡•á! JARVIS API ready! üòä',
        'time_query': f'‡§∏‡§Æ‡§Ø: {datetime.datetime.now().strftime("%H:%M")} IST',
        'goodbye': '‡§Ö‡§≤‡§µ‡§ø‡§¶‡§æ! API ‡§¨‡§Ç‡§¶‡•§ üòä',
        'error': 'Model loading error!',
        'unknown': '‡§∏‡§Æ‡§ù‡§æ ‡§®‡§π‡•Ä‡§Ç... Try "hello" or "‡§®‡§Æ‡§∏‡•ç‡§§‡•á"'
    }
    return responses.get(intent, f'Intent "{intent}" ‚Üí Under development...')

@app.on_event("startup")
async def startup_event():
    load_models()

@app.post("/predict")
async def predict(query: Query):
    intent = predict_intent(query.text)
    response = handle_intent(intent)
    return {
        "input": query.text,
        "intent": intent, 
        "response": response,
        "timestamp": datetime.datetime.now().isoformat()
    }

@app.get("/")
async def root():
    return {"message": "ü§ñ JARVIS API v1.0 LIVE!", "endpoints": ["/predict", "/docs"]}

@app.get("/health")
async def health():
    models_ok = model is not None and vectorizer is not None
    return {"status": "healthy", "models_loaded": models_ok}
